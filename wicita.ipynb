{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a0dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc as auc_score\n",
    "\n",
    "# Python Class for embedding extraction\n",
    "from extraction import WordEmbeddingExtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8818a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wicita_path = '/project/folder/' # Use your project folder\n",
    "embs_path = '/project/folder/embeddings' # Folder where embeddings will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6772fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatname(txt):\n",
    "    return txt.replace('.jsonl', '.new.jsonl')\n",
    "\n",
    "def change_format(data_filename, binary_filename, ranking_filename, test=None):\n",
    "    dataset = list()\n",
    "    gold = list()\n",
    "    ranking = list()\n",
    "    \n",
    "    lines = open(data_filename.replace('/binary/', '/ranking/'), mode='r', encoding='utf-8').readlines()\n",
    "    for i, line in enumerate(open(data_filename, mode='r', encoding='utf-8')):\n",
    "        pair = json.loads(line)\n",
    "        dataset.append(dict(id=pair['id'], \n",
    "                            lemma=pair[\"lemma\"] if test != 'eng' else pair[\"lemma1\"],\n",
    "                            sent=pair['sentence1'],\n",
    "                            start=pair['start1'],\n",
    "                            end=pair['end1']))\n",
    "        \n",
    "        dataset.append(dict(id=pair['id'], \n",
    "                            lemma=pair[\"lemma\"] if test != 'eng' else pair[\"lemma2\"],\n",
    "                            sent=pair['sentence2'],\n",
    "                            start=pair['start2'],\n",
    "                            end=pair['end2']))\n",
    "        \n",
    "        if test is None:\n",
    "            gold.append(f'{pair[\"label\"]}\\n')\n",
    "            ranking.append(f'{json.loads(lines[i])[\"score\"]}\\n')\n",
    "    \n",
    "    pd.DataFrame(dataset).to_json(formatname(data_filename), orient='records', lines=True)\n",
    "    \n",
    "    if test is None:\n",
    "        open(binary_filename, mode='w').writelines(gold)\n",
    "        open(ranking_filename, mode='w').writelines(ranking)\n",
    "\n",
    "def extract(model, dataset, batch_size=32, max_length=512, agg_sub_words='mean', layers=12):\n",
    "    embs = WordEmbeddingExtraction(model).extract(dataset=dataset, \n",
    "                                                  batch_size=batch_size,\n",
    "                                                  max_length=max_length, \n",
    "                                                  agg_sub_words=agg_sub_words,\n",
    "                                                  layers=layers)\n",
    "    split_embs = defaultdict(dict)\n",
    "\n",
    "    for l in range(1, 12+1):\n",
    "        E = embs[l]\n",
    "        E1, E2 = list(), list()\n",
    "        for i in range(0, E.shape[0], 2):\n",
    "            E1.append(E[i])\n",
    "            E2.append(E[i+1])\n",
    "\n",
    "        split_embs['sent1'][l]=torch.stack(E1)\n",
    "        split_embs['sent2'][l]=torch.stack(E2)\n",
    "    \n",
    "    return dict(split_embs)\n",
    "\n",
    "def compute_scores(embeddings, layers=12):\n",
    "    scores = defaultdict(list)\n",
    "\n",
    "    n_pairs = embeddings['sent1'][1].shape[0]\n",
    "    \n",
    "    for i in range(n_pairs):\n",
    "        embs_t1, embs_t2 = list(), list()\n",
    "\n",
    "        for j in range(1, layers + 1):\n",
    "            embs_t1_lj, embs_t2_lj = embeddings['sent1'][j][i].cpu(), embeddings['sent2'][j][i].cpu()\n",
    "            embs_t1.append(embs_t1_lj)\n",
    "            embs_t2.append(embs_t2_lj)\n",
    "            \n",
    "            # cosine similarity\n",
    "            cd = cdist([embs_t1_lj.numpy()], [embs_t2_lj.numpy()], metric='cosine')[0][0]\n",
    "            scores[f'CS{j}'].append(1 - cd)\n",
    "        \n",
    "        scores[f'CS_AVG'].append(1 - cdist([torch.stack(embs_t1[-4:]).mean(axis=0).numpy()], \n",
    "                                           [torch.stack(embs_t2[-4:]).mean(axis=0).numpy()], \n",
    "                                           metric='cosine')[0][0])\n",
    "        \n",
    "        # Cosine Distance and Similarity Matrix between embeddings of different layers\n",
    "        cd_matrix = cdist([e.numpy() for e in embs_t1], [e.numpy() for e in embs_t2], metric='cosine')\n",
    "        cs_matrix = 1-cdist([e.numpy() for e in embs_t1], [e.numpy() for e in embs_t2], metric='cosine')\n",
    "        cs_matrix_F = cs_matrix[:4, :4]\n",
    "        cs_matrix_M = cs_matrix[4:8, 4:8]\n",
    "        cs_matrix_L = cs_matrix[-4:, -4:]\n",
    "        \n",
    "        # Cond\n",
    "        cond = np.linalg.cond(cs_matrix, 'fro')\n",
    "        cond_F = np.linalg.cond(cs_matrix_F, 'fro')\n",
    "        cond_M = np.linalg.cond(cs_matrix_M, 'fro')\n",
    "        cond_L = np.linalg.cond(cs_matrix_L, 'fro')\n",
    "        \n",
    "        scores['-COND'].append(-cond_cs)\n",
    "        scores['-COND_L'].append(-cond_L)\n",
    "        scores['-COND_M'].append(-cond_M)\n",
    "        scores['-COND_F'].append(-cond_F)\n",
    "\n",
    "    for s in scores:\n",
    "        scores[s] = np.array(scores[s])\n",
    "\n",
    "    return scores\n",
    "\n",
    "def best_threshold(y_true: np.array, y: np.array, func='accuracy') -> tuple:\n",
    "    \"\"\"\n",
    "    Calculates the accuracy/f1 score for a binary classification problem.\n",
    "    The function first calculates the False Positive Rate (FPR), True Positive Rate (TPR), and Thresholds using the\n",
    "    roc_curve function from Scikit-learn. Next, it calculates the accuracy score for each threshold value and returns\n",
    "    the maximum accuracy score and its corresponding threshold value.\n",
    "\n",
    "    Args:\n",
    "        y(np.array): array containing predicted values\n",
    "        y_true(np.array): array containing ground truth values.\n",
    "    Returns:\n",
    "        acc, thr\n",
    "    \"\"\"\n",
    "\n",
    "    # False Positive Rate - True Positive Rate\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y)\n",
    "\n",
    "    scores = []\n",
    "    for thresh in thresholds:\n",
    "        if func == 'accuracy':\n",
    "            scores.append(accuracy_score(y_true, [m >= thresh for m in y]))\n",
    "        elif func == 'f1':\n",
    "            scores.append(f1_score(y_true, [m >= thresh for m in y], average='weighted'))\n",
    "\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    # Max accuracy\n",
    "    max_ = scores.max()\n",
    "\n",
    "    # Threshold associated to the maximum accuracy\n",
    "    max_threshold = thresholds[scores.argmax()]\n",
    "\n",
    "    return round(float(max_), 3), max_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e9a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugginface models\n",
    "models = {'it': 'dbmdz/bert-base-italian-cased',\n",
    "          'm': 'bert-base-multilingual-cased',\n",
    "          'xl': 'xlm-roberta-base'}\n",
    "\n",
    "# Wicita datasets\n",
    "datasets = {'train': f'{wicita_path}/binary/train.jsonl',\n",
    "            'dev': f'{wicita_path}/binary/dev.jsonl',\n",
    "            'test': f'{wicita_path}/binary/test.jsonl',\n",
    "            'test_eng': f'{wicita_path}/binary/test_eng.jsonl'}\n",
    "\n",
    "# filename of new gold data files\n",
    "it_binary_golds = {'train': f'{wicita_path}/train_gold_binary.txt',\n",
    "                   'dev': f'{wicita_path}/dev_gold_binary.txt',\n",
    "                   'test': f'{wicita_path}/test_gold_binary.txt',\n",
    "                   'test_eng': f'{wicita_path}/test_eng_gold_binary.txt'}\n",
    "\n",
    "it_ranking_golds = {'train': f'{wicita_path}/train_gold_ranking.txt',\n",
    "                   'dev': f'{wicita_path}/dev_gold_ranking.txt',\n",
    "                   'test': f'{wicita_path}/test_gold_ranking.txt',\n",
    "                   'test_eng': f'{wicita_path}/test_eng_gold_ranking.txt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7f4c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data format: make sure you downloaded the data and created the directory\n",
    "change_format(datasets['train'], it_binary_golds['train'], it_ranking_golds['train'])\n",
    "change_format(datasets['dev'], it_binary_golds['dev'], it_ranking_golds['dev'])\n",
    "change_format(datasets['test'], it_binary_golds['test'], it_ranking_golds['test'], test='it')\n",
    "change_format(datasets['test_eng'], it_binary_golds['test_eng'], it_ranking_golds['test_eng'], test='eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bfbad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect embeddings for each model\n",
    "for m in list(models):\n",
    "    train_embs = extract(models[m], formatname(datasets['train']))\n",
    "    dev_embs = extract(models[m], formatname(datasets['dev']))\n",
    "    test_embs = extract(models[m], formatname(datasets['test']))\n",
    "    test_eng_embs = extract(models[m], formatname(datasets['test_eng']))\n",
    "    \n",
    "    torch.save(train_embs, f'{embs_path}/{m}_train.pt')\n",
    "    torch.save(dev_embs, f'{embs_path}/{m}_dev.pt')\n",
    "    torch.save(test_embs, f'{embs_path}/{m}_test.pt')\n",
    "    torch.save(test_eng_embs, f'{embs_path}/{m}_test_eng.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a model\n",
    "model = 'xl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1702d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "train_embs = torch.load(f'{embs_path}/{model}_train.pt')\n",
    "dev_embs = torch.load(f'{embs_path}/{model}_dev.pt')\n",
    "test_embs = torch.load(f'{embs_path}/{model}_test.pt')\n",
    "test_eng_embs = torch.load(f'{embs_path}/{model}_test_eng.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute score [cosine similarities, and condition number]\n",
    "scores_train = compute_scores(train_embs)\n",
    "scores_dev = compute_scores(dev_embs)\n",
    "scores_test = compute_scores(test_embs)\n",
    "scores_test_eng = compute_scores(test_eng_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075fe7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary and Ranking ground truth\n",
    "bin_gold_train = np.array([eval(i.strip()) for i in open(it_binary_golds['train'], mode='r').readlines()])\n",
    "bin_gold_dev = np.array([eval(i.strip()) for i in open(it_binary_golds['dev'], mode='r').readlines()])\n",
    "rank_gold_train = np.array([eval(i.strip()) for i in open(it_ranking_golds['train'], mode='r').readlines()])\n",
    "rank_gold_dev = np.array([eval(i.strip()) for i in open(it_ranking_golds['dev'], mode='r').readlines()])\n",
    "\n",
    "#gold_test = np.array([eval(i.strip()) for i in open(it_binary_golds['test'], mode='r').readlines()])\n",
    "#gold_test_eng = np.array([eval(i.strip()) for i in open(it_binary_golds['test_eng'], mode='r').readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2669df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_eval_binary(Y_train, Y_test, scores_train, scores_test):\n",
    "    stats=defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for s in scores_train:\n",
    "        acc, thr=best_threshold(Y_train, scores_train[s], 'accuracy')\n",
    "        stats[s]['score_train'] = np.array([int(i>=thr) for i in scores_train[s]])\n",
    "        stats[s]['score_test'] = np.array([int(i>=thr) for i in scores_test[s]])\n",
    "        stats[s]['acc_train']=acc\n",
    "        stats[s]['acc_test']=accuracy_score(Y_test, [int(i>=thr) for i in scores_test[s]])\n",
    "        \n",
    "        f1, thr=best_threshold(Y_train, scores_train[s], 'f1')\n",
    "        stats[s]['f_wscore_train']=f1\n",
    "        stats[s]['f_wscore_test']=f1_score(Y_test, [int(i>=thr) for i in scores_test[s]])\n",
    "        stats[s]['thr']=thr\n",
    "\n",
    "    df = pd.DataFrame(stats).T.reset_index().rename(columns={'index':'measure'})\n",
    "    df = df.sort_values(by=['acc_test', 'measure'], ascending=False)\n",
    "    return df\n",
    "\n",
    "def full_eval_ranking(Y_train, Y_test, scores_train, scores_test):\n",
    "    stats=defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for s in scores_train:\n",
    "        stats[s]['score_train'] = scores_train[s]\n",
    "        stats[s]['score_test'] = scores_test[s]\n",
    "        corr, pvalue=spearmanr(Y_train, scores_train[s])\n",
    "        stats[s]['corr_train']=corr\n",
    "        stats[s]['pvalue_train']=pvalue\n",
    "        corr, pvalue=spearmanr(Y_test, scores_test[s])\n",
    "        stats[s]['corr_test']=corr\n",
    "        stats[s]['pvalue_test']=pvalue\n",
    "\n",
    "    df = pd.DataFrame(stats).T.reset_index().rename(columns={'index':'measure'})\n",
    "    df = df.sort_values(by=['corr_test', 'measure'], ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6137bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_t = full_eval_binary(bin_gold_train, bin_gold_dev, scores_train, scores_dev)\n",
    "ranking_t = full_eval_ranking(rank_gold_train, rank_gold_dev, scores_train, scores_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b19ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_d = full_eval_binary(bin_gold_dev, bin_gold_train, scores_dev, scores_train)\n",
    "ranking_d = full_eval_ranking(rank_gold_dev, rank_gold_train, scores_dev, scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c53ca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = ['-COND', '-COND_L', '-COND_M', '-COND_F', \n",
    "            'CS10', 'CS_AVG', 'CS9', 'CS8', 'CS11', 'CS7',\n",
    "            'CS6', 'CS5', 'CS12', 'CS4', 'CS3', 'CS1', 'CS2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d65a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_gold_tot = np.concatenate([bin_gold_dev, bin_gold_train])\n",
    "rank_gold_tot = np.concatenate([rank_gold_dev, rank_gold_train])\n",
    "scores_tot = {m: np.concatenate([scores_dev[m], scores_train[m]]) for m in measures}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf426a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "stats=defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "n_splits = 100\n",
    "n_example_train = 2000\n",
    "mask = list(range(0, bin_gold_tot.shape[0]))\n",
    "\n",
    "for i in tqdm(range(n_splits)):\n",
    "    random.shuffle(mask)\n",
    "    \n",
    "    # Gold true: Train - Test\n",
    "    bin_cv_train_gold = bin_gold_tot[mask[:n_example_train]]\n",
    "    bin_cv_test_gold = bin_gold_tot[mask[n_example_train:]]\n",
    "    rank_cv_train_gold = rank_gold_tot[mask[:n_example_train]]\n",
    "    rank_cv_test_gold = rank_gold_tot[mask[n_example_train:]]\n",
    "    \n",
    "    # Predictions: Train - Test\n",
    "    cv_scores_train = {m: scores_tot[m][mask[:n_example_train]] for m in measures}\n",
    "    cv_scores_test = {m: scores_tot[m][mask[n_example_train:]] for m in measures}\n",
    "    \n",
    "    for s in cv_scores_train:\n",
    "        corr, pvalue=spearmanr(rank_cv_train_gold, cv_scores_train[s])\n",
    "        stats[s]['corr_train'].append(corr)\n",
    "        stats[s]['pvalue_train'].append(pvalue)\n",
    "        \n",
    "        corr, pvalue=spearmanr(rank_cv_test_gold, cv_scores_test[s])\n",
    "        stats[s]['corr_test'].append(corr)\n",
    "        stats[s]['pvalue_test'].append(pvalue)\n",
    "        \n",
    "        _, thr=best_threshold(bin_cv_train_gold, cv_scores_train[s], func='f1')\n",
    "        train_preds = [int(i>=thr) for i in cv_scores_train[s]]\n",
    "        pr, re, f1, _ = precision_recall_fscore_support(bin_cv_train_gold, train_preds, average='weighted')\n",
    "        stats[s]['pr_train'].append(pr)\n",
    "        stats[s]['re_train'].append(re)\n",
    "        stats[s]['f1_train'].append(f1)\n",
    "        \n",
    "        test_preds = [int(i>=thr) for i in cv_scores_test[s]]\n",
    "        pr, re, f1, _ = precision_recall_fscore_support(bin_cv_test_gold, test_preds, average='weighted')\n",
    "        stats[s]['pr_test'].append(pr)\n",
    "        stats[s]['re_test'].append(re)\n",
    "        stats[s]['f1_test'].append(f1)\n",
    "            \n",
    "        stats[s]['thr'].append(thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Development Stats\n",
    "df_stats=defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for m in stats:\n",
    "    for s in list(stats[m]):\n",
    "        df_stats[m][s] = np.array(stats[m][s]).mean()\n",
    "\n",
    "df = pd.DataFrame(df_stats).T.round(3).sort_index()\n",
    "df.sort_values('f1_test', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009877f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(filename, goldname, scores, output):\n",
    "    res = list()\n",
    "    lines = open(filename, mode='r', encoding='utf-8').readlines()\n",
    "    for k, i in enumerate(range(0, len(lines), 2)):\n",
    "        if 'binary' in output:\n",
    "            res.append(dict(id=json.loads(lines[i])['id'],\n",
    "                 label=scores[k]))\n",
    "        else:\n",
    "            res.append(dict(id=json.loads(lines[i])['id'],\n",
    "             score=scores[k]))\n",
    "    \n",
    "    pd.DataFrame(res).to_json(output, orient='records', lines=True)\n",
    "    lines = open(output).readlines()\n",
    "    lines = lines[:-1] + [lines[-1].strip()]\n",
    "    open(output, mode='w').writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5559474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = '-COND_M'\n",
    "thr= -1195.522\n",
    "\n",
    "submit(formatname(datasets['dev']), it_binary_golds['dev'], [int(i>=thr) for i in scores_dev[measure]], f\"{wicita_path}binary_dev.jsonl\")\n",
    "submit(formatname(datasets['dev']), it_ranking_golds['dev'], scores_dev[measure], f\"{wicita_path}ranking_dev.jsonl\")\n",
    "\n",
    "submit(formatname(datasets['train']), it_binary_golds['train'],  [int(i>=thr) for i in scores_train[measure]], f\"{wicita_path}binary_train.jsonl\")\n",
    "submit(formatname(datasets['train']), it_ranking_golds['train'], scores_train[measure], f\"{wicita_path}ranking_train.jsonl\")\n",
    "\n",
    "submit(formatname(datasets['test']), it_binary_golds['test'], [int(i>=thr) for i in scores_test[measure]], f\"{wicita_path}binary.jsonl\")\n",
    "submit(formatname(datasets['test']), it_ranking_golds['test'], scores_test[measure], f\"{wicita_path}ranking.jsonl\")\n",
    "\n",
    "submit(formatname(datasets['test_eng']), it_binary_golds['test_eng'], [int(i>=thr) for i in scores_test_eng[measure]], f\"{wicita_path}binary_eng.jsonl\")\n",
    "submit(formatname(datasets['test_eng']), it_ranking_golds['test_eng'], scores_test_eng[measure], f\"{wicita_path}ranking_eng.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
